{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a realizar operaciones de **clustering** (segmentación) de datos.\n",
    "La idea es encontrar una estructura dentro de un dataset donde originalmente no la había.\n",
    "No se tiene un objetivo de predicción (se trata **aprendizaje no supervisado**), sino de uno de entendimiento de los datos a través del particionamiento del dataset en grupos de instancias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 1. K-Means con datos sintéticos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder entender como se utilizan los algoritmos de clustering, vamos inicialmente a crear un dataset sintético con datos ficticios que nos permita ilustrar los aspectos de llamado a los métodos de python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a generar y visualizar en un plot 300 puntos aleatorios distribuidos alrededor de 4 centros en un espacio bidimensional, con una desviación estándar de 0.7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, silhouette_samples, silhouette_score, calinski_harabaz_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "X, grupo = make_blobs(n_samples=300, centers=4, cluster_std=0.8, random_state=0)\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En X van a quedar las coordenadas de los puntos y en **grupo** los clusters originales a los que pertencen los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grupo[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo de K-Means recibe como parámetro el número de clusters que se buscan (hay que sepecificarlo ya que no lo determina automáticamente). Como sabemos que los datos sintéticos se crearon con 4 grupos, vamos a analizar si K-Means los logra detectar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4, random_state=0)\n",
    "kmeans.fit(X)\n",
    "grupo_kmeans = kmeans.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a plotear los clusters encontrados con diferentes colores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=grupo_kmeans, s=50, cmap='viridis')\n",
    "\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora comparemos los grupos encontrados con los reales, utilizando una matriz de confusión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(grupo, grupo_kmeans)\n",
    "plt.imshow(cm, cmap=plt.cm.Blues)\n",
    "plt.title(\"Comparación entre los clusters reales y los descubiertos por K-Means\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(4)\n",
    "plt.xticks(tick_marks, ['0','1','2','3'])\n",
    "plt.yticks(tick_marks, ['0','1','2','3'])\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En las filas encontramos los grupos reales y en las columnas los de K-Means. Encontramos 6 errores, todos asociados por k-means al grupo 0 cuando eran de alguno de los otros 3 grupos.\n",
    "\n",
    "Hay que tener en cuenta que el orden de los nombres de los grupos generados puede no conincidir con el orden de los grupos encontrados por K-Means, como es el caso aquí.\n",
    "\n",
    "Lo que vemos es que parece haber una concordancia entre los clusters encontrados por K-Means y los reales: los grupos 0, 1, 2 y 3 de k-means corresponden a los grupos 1, 0, 2, y 3 encontrados por K-Means, respectivamente.\n",
    "\n",
    "Vamos a cambiar el orden de los clusters de k-means para poder entender mejor los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "traducir = [1, 0, 2, 3]\n",
    "\n",
    "grupo_kmeans_reorg = []\n",
    "for g_k, g in zip(grupo_kmeans, grupo):\n",
    "    grupo_kmeans_reorg.append(traducir[g_k])\n",
    "print(grupo_kmeans_reorg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos hacer esto de una manera mas breve utilizando una de las particularidades de Python: List comprehensions, que permite resumir operaciones simples realizadas dentro de un ciclo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grupo_kmeans_reorg = [traducir[g_k] for g_k in grupo_kmeans]\n",
    "print(grupo_kmeans_reorg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(grupo, grupo_kmeans_reorg)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(grupo, grupo_kmeans_reorg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos gráficamente cuáles son los registros que se asocian a un grupo diferente a su original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diferentes = []\n",
    "for (x0, x1), g, gk in zip(X, grupo, grupo_kmeans_reorg):\n",
    "    if g!=gk:\n",
    "        diferentes.append([x0, x1])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diferentes = np.array(diferentes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=grupo_kmeans_reorg, s=50, cmap='viridis')\n",
    "\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);\n",
    "\n",
    "plt.scatter(diferentes[:, 0], diferentes[:, 1], c='red', marker=\"x\", s=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preguntas**:\n",
    "Comparamos los grupos creados por el clustering con los originales (esto se llama \"clasificación no supervisada\")\n",
    "1. Expliquen la primera matriz de confusión obtenida y el por qué fue necesario recodificar los segmentos obtenidos por el clustering.\n",
    "2. ¿Qué tan bien puede K-Means encontrar las categorías originales en terminos de accuracy?\n",
    "3. ¿Tiene sentido crear un test set para un clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 2. K-Means con datos reales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('01 - ComprasClientes.csv', na_values=\".\")\n",
    "print(data.shape)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Qué problemas saltan a la vista al inspeccionar los datos?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Las variables Channel y Region tienen tipo int64, cuando en realidad codifican categorías de canales y de regiones. Es necesario cambiar sus tipos.\n",
    "1. Tenemos en todas las variables de consumo valores anormalmente grandes que pueden considerarse excepciones en el mejor de los casos (anomalías o errores de captura en el peor de los casos). Hay que identificar los registros en cuestión y evaluar la posibilidad de descartarlos pues pueden influenciar negativamente muchos de los modelos que se pueden aprender a partir de los datos.\n",
    "1. Las escalas de las variables que denotan los montos consumidos de cada tipo de productos son muy disparejas. Es necesario normalizar los datos ya que de no hacerlo se otorgaría una importancia demasiado desmedida a variables como Fresh casi que ignorando variables como Delicatessen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arreglamos primero los tipos de datos incorrectos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Channel = data.Channel.astype(str)\n",
    "data.Region = data.Region.astype(str)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de normalizar es necesario limpiar las excepciones o anomalías con valores o muy grandes o muy pequeñas. Vamos a analizar las variables numéricas a partir de diagramas de cajas y bigotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "data.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que hay valores muy importantes en todas las variables. Si contamos los puntos individuales más elevados podemos identificar 6 o menos puntos que sobrepasan la mayoría de los demás.\n",
    "Puede que algunos de los puntos excepcionales en las diferentes variables correspondan a los mismos individuos. Vamos a identificar los top 6 de valores mas importantes en cada tipo de producto y no los vamos a considerar en los análisis siguientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = data.sort_values(['Fresh'], ascending=False)\n",
    "print(\"Excepciones de Fresh: \", np.sort(temp[0:6].index.get_values()))\n",
    "indicesAQuitar = temp[0:6].index.get_values()\n",
    "\n",
    "temp = data.sort_values(['Milk'], ascending=False)\n",
    "print(\"Excepciones de Milk: \", np.sort(temp[0:6].index.get_values()))\n",
    "indicesAQuitar = np.union1d(indicesAQuitar, temp[0:6].index.get_values())\n",
    "\n",
    "\n",
    "temp = data.sort_values(['Grocery'], ascending=False)\n",
    "print(\"Excepciones de Grocery: \", np.sort(temp[0:6].index.get_values()))\n",
    "indicesAQuitar = np.union1d(indicesAQuitar, temp[0:6].index.get_values())\n",
    "\n",
    "temp = data.sort_values(['Frozen'], ascending=False)\n",
    "print(\"Excepciones de Frozen: \", np.sort(temp[0:6].index.get_values()))\n",
    "indicesAQuitar = np.union1d(indicesAQuitar, temp[0:6].index.get_values())\n",
    "\n",
    "temp = data.sort_values(['Detergents_Paper'], ascending=False)\n",
    "print(\"Excepciones de Detergents_Paper: \", np.sort(temp[0:6].index.get_values()))\n",
    "indicesAQuitar = np.union1d(indicesAQuitar, temp[0:6].index.get_values())\n",
    "\n",
    "temp = data.sort_values(['Delicassen'], ascending=False)\n",
    "print(\"Excepciones de Delicassen: \", np.sort(temp[0:6].index.get_values()))\n",
    "indicesAQuitar = np.union1d(indicesAQuitar, temp[0:6].index.get_values())\n",
    "\n",
    "indicesAQuitar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos 22 registros identificados como excepciones. Vemos que algunos tienen valores excepcionales según diferentes tipos de consumo (23, 47, 61, 65, 85, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDepurado = data.loc[~data.index.isin(indicesAQuitar)]\n",
    "dataDepurado.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ahora a normalizar los datos para que todas las variables tengan la misma importancia. Solo vamos a considerar los datos numéricos, por lo que no incluimos las variables Channel y Region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataStd = pd.DataFrame(preprocessing.scale(dataDepurado.iloc[:,2:]))\n",
    "dataStd.columns=dataDepurado.columns[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataStd.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataStd.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Con un k de 3, realice un clustering por K-Means (utilicen random_state=0).**\n",
    "\n",
    "**Agregue una columna \"Cluster\" con el segmento correspondiente (0, 1, o 2) al dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3, random_state=0, n_init=10)\n",
    "kmeans.fit(dataStd.iloc[:,0:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método KMeans en scikit-learn permite definir los valores de ciertos parámetros que controlan la ejecución del algoritmo de clustering. Nos interesan particularmente:\n",
    "- **n_clusters**: número de clusters que se desean (el parámetro \"K\"). Por defecto es 8.\n",
    "- **init**: el método de inicialización de los centroides. Por defecto es \"k-means++\". Otros valores son \"random\" o un array con los centroides iniciales\n",
    "- **n_init**: número de inicializaciones diferentes a ensayar para evitar llegar a un óptimo local. Por defecto es 10\n",
    "- **max_iter**: Máximo número de iteraciones que se esparará para llegar a convergencia. Por defecto es 300.\n",
    "- **tol**: tolerancia para determinar que se ha llegado o no a convergenia con respecto a la reducción del WSS (interia). Por defecto es 0.0001\n",
    "- **random_state**: semilla de inicialización del generador pseudo-aleatorio para poder reproducir los resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objeto resultado del KMeans después de lanzado el ajuste del algoritmo consta de diferentes valores de salida:\n",
    "- **cluster_centers_**: los centroides finales de los clusters.\n",
    "- **labels_**: los clusters a los cuales termina perteneciendo cada instancia del set de aprendizaje.\n",
    "- **interia_**: el WSS final.\n",
    "- **n_iter_**: el número de iteraciones que tomó llegar a convergencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Le tomó a KMeans\", kmeans.n_iter_, \"iteraciones llegar a convergencia, con un WSS final de:n\",\n",
    "      kmeans.inertia_, \"y los centroides siguientes:\", kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos utilizar el objeto kmeans resultante como modelo de clasificación, al que a través del método *predict* se le puede enviar un dataset para evaluar y obtener los clusters a los que pertenecen. Por ejemplo, utilicemoslo para clasificar los mismos ejemplos de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = kmeans.predict(dataStd.iloc[:, 0:6])\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=Counter(clusters)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos 3 clusters de 238, 93 y 87 instancias cada uno.\n",
    "Agregamos una columna al dataframe con los datos analizados que indique a que cluster pertenece cada registro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataStd.loc[:,'Cluster'] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataStd.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataStd[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 3. Interpretación de los clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretación de los clusters, con k =3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a tratar de entender cuales son las características de los registros que los componen. Para ello vamos a ver gráficos de densidad que permitan identificar las predilecciones de compras de los clientes que pertenecen a cada cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_num = ['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,15))\n",
    "i=1\n",
    "for var in var_num:\n",
    "    ax = fig.add_subplot(math.ceil(len(var_num)/2), 2, i)\n",
    "    sns.kdeplot(dataStd.loc[dataStd.Cluster==0][var], shade=True, color='r', ax=ax);\n",
    "    sns.kdeplot(dataStd.loc[dataStd.Cluster==1][var], shade=True, color='g', ax=ax);\n",
    "    sns.kdeplot(dataStd.loc[dataStd.Cluster==2][var], shade=True, color='b', ax=ax);\n",
    "    plt.title(var)\n",
    "    plt.legend(['Cluster 0', 'Cluster 1', 'Cluster 2'])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veámoslos en scatterplots para entender mejor las diferencias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,15))\n",
    "colorPalette = [\"r\", \"g\", \"b\"]\n",
    "ax = fig.add_subplot(2, 2, 1)\n",
    "sns.scatterplot(x=\"Fresh\", y=\"Milk\", hue=\"Cluster\", data=dataStd, ax=ax, palette=colorPalette, s=100, alpha=0.5)\n",
    "plt.title(\"Fresh vs. Milk\")\n",
    "ax = fig.add_subplot(2, 2, 2)\n",
    "sns.scatterplot(x=\"Frozen\", y=\"Grocery\", hue=\"Cluster\", data=dataStd, ax=ax, palette=colorPalette, s=100, alpha=0.5)\n",
    "plt.title(\"Frozen vs. Grocery\")\n",
    "ax = fig.add_subplot(2, 2, 3)\n",
    "sns.scatterplot(x=\"Delicassen\", y=\"Detergents_Paper\", hue=\"Cluster\", data=dataStd, ax=ax, palette=colorPalette, s=100, alpha=0.5)\n",
    "plt.title(\"Delicassen vs. Detergents_Paper\")\n",
    "ax = fig.add_subplot(2, 2, 4)\n",
    "sns.scatterplot(x=\"Fresh\", y=\"Frozen\", hue=\"Cluster\", data=dataStd, ax=ax, palette=colorPalette, s=100, alpha=0.5)\n",
    "plt.title(\"Fresh vs. Frozen\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cómo se puede distinguir entre:\n",
    "- los rojos y los demás\n",
    "- los verdes y los demás\n",
    "- los azules y los demás"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos interpretar entonces los clusters de esta manera:\n",
    "- Cluster 0 (Rojo, 238 registros). Tiene valores:\n",
    "    - Altos : Milk, Grocery, Detergents_Paper\n",
    "    - Medios: Delicassen\n",
    "    - Bajos : Fresh, Frozen\n",
    "- Cluster 1 (Verde, 93 registros). Tiene valores:\n",
    "    - Altos : \n",
    "    - Medios: \n",
    "    - Bajos : Fresh, Milk, Grocery, Frozen, Detergents_Paper, Delicassen\n",
    "- Cluster 2 (Azul, 87 registros). Tiene valores:\n",
    "    - Altos : Fresh, Frozen\n",
    "    - Medios: Delicassen\n",
    "    - Bajos : Milk , Grocery, Detergents_Paper\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Qué podemos decir de los 3 clusters, qué adjetivo les darían para describirlos?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretación de los clusters, con k = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repetimos el análisis con k=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4, random_state=0, n_init=10)\n",
    "kmeans.fit(dataStd.iloc[:, 0:6])\n",
    "clusters = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataStd['Cluster']= clusters\n",
    "counter=Counter(clusters)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,15))\n",
    "i=1\n",
    "for var in var_num:\n",
    "    ax = fig.add_subplot(math.ceil(len(var_num)/2), 2, i)\n",
    "    sns.kdeplot(dataStd.loc[dataStd.Cluster==0][var], shade=True, color='r', ax=ax);\n",
    "    sns.kdeplot(dataStd.loc[dataStd.Cluster==1][var], shade=True, color='g', ax=ax);\n",
    "    sns.kdeplot(dataStd.loc[dataStd.Cluster==2][var], shade=True, color='b', ax=ax);\n",
    "    sns.kdeplot(dataStd.loc[dataStd.Cluster==3][var], shade=True, color='y', ax=ax);\n",
    "    plt.title(var)\n",
    "    plt.legend(['Cluster 0', 'Cluster 1', 'Cluster 2', 'Cluster 3'])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamoslos en scatterplots para entender mejor las diferencias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,15))\n",
    "colorPalette = [\"r\", \"g\", \"b\", \"y\"]\n",
    "ax = fig.add_subplot(2, 2, 1)\n",
    "sns.scatterplot(x=\"Fresh\", y=\"Milk\", hue=\"Cluster\", data=dataStd, ax=ax, palette=colorPalette, s=100, alpha=0.5)\n",
    "plt.title(\"Fresh vs. Milk\")\n",
    "ax = fig.add_subplot(2, 2, 2)\n",
    "sns.scatterplot(x=\"Frozen\", y=\"Grocery\", hue=\"Cluster\", data=dataStd, ax=ax, palette=colorPalette, s=100, alpha=0.5)\n",
    "plt.title(\"Frozen vs. Grocery\")\n",
    "ax = fig.add_subplot(2, 2, 3)\n",
    "sns.scatterplot(x=\"Delicassen\", y=\"Detergents_Paper\", hue=\"Cluster\", data=dataStd, ax=ax, palette=colorPalette, s=100, alpha=0.5)\n",
    "plt.title(\"Delicassen vs. Detergents_Paper\")\n",
    "ax = fig.add_subplot(2, 2, 4)\n",
    "sns.scatterplot(x=\"Fresh\", y=\"Frozen\", hue=\"Cluster\", data=dataStd, ax=ax, palette=colorPalette, s=100, alpha=0.5)\n",
    "plt.title(\"Fresh vs. Frozen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cómo se puede distinguir entre:\n",
    "- los rojos y los demás\n",
    "- los verdes y los demás\n",
    "- los azules y los demás\n",
    "- los amarillos y los demás"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretación de los clusters, con k =2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repetimos el análisis con k=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2, random_state=0, n_init=10)\n",
    "kmeans.fit(dataStd.iloc[:, 0:6])\n",
    "clusters = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataStd['Cluster']= clusters\n",
    "counter=Counter(clusters)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,15))\n",
    "i=1\n",
    "for var in var_num:\n",
    "    ax = fig.add_subplot(math.ceil(len(var_num)/2), 2, i)\n",
    "    sns.kdeplot(dataStd.loc[dataStd.Cluster==0][var], shade=True, color='r', ax=ax);\n",
    "    sns.kdeplot(dataStd.loc[dataStd.Cluster==1][var], shade=True, color='b', ax=ax);\n",
    "    plt.title(var)\n",
    "    plt.legend(['Cluster 0', 'Cluster 1'])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que con K=2, las compras en Delicatessen y Frozen no sirven para discriminar entre los 2 grupos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamoslos en scatterplots para entender mejor las diferencias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,7))\n",
    "colorPalette = [\"r\", \"b\"]\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "sns.scatterplot(x=\"Fresh\", y=\"Milk\", hue=\"Cluster\", data=dataStd, ax=ax, palette=colorPalette, s=100, alpha=0.5)\n",
    "plt.title(\"Fresh vs. Milk\")\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "sns.scatterplot(x=\"Detergents_Paper\", y=\"Grocery\", hue=\"Cluster\", data=dataStd, ax=ax, palette=colorPalette, s=100, alpha=0.5)\n",
    "plt.title(\"Detergents_Paper vs. Grocery\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que con K=2, las compras en Delicatessen y Frozen no sirven para discriminar entre los 2 grupos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos interpretar entonces los clusters de esta manera:\n",
    "- Cluster 0 (Rojo, 330 registros). Tiene valores:\n",
    "    - Altos : Milk, Grocery, Detergents_Paper\n",
    "    - Medios: \n",
    "    - Bajos : Fresh\n",
    "- Cluster 1 (Azul, 88 registros). Tiene valores:\n",
    "    - Altos : Fresh\n",
    "    - Medios: \n",
    "    - Bajos : Milk , Grocery, Detergents_Paper    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Son mucho mas separables las categorías cuando K=2, pero puede que la información no sea suficientemente rica para las acciones que se deseen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 4. Determinación del K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "En el atributo *inertia_* queda el valor de la suma de las distancias cuadráticas entre cada punto y el centro del cluster al que pertenece (el **WSS** - Within Sum of Squares, también llamado más genéricamente **SSE** - Sum of Sqaured Errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.inertia_\n",
    "kmeans.init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de los valores de WSS se puede crear el plot a partir del cual se aplica la técnica del codo, creando un clustering para diferentes valores de K. Veamos, según el método del codo, cual sería el valor del K en este conjunto de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WSSs = []\n",
    "for i in range(1,15) :\n",
    "    km = KMeans(n_clusters=i, random_state=0)\n",
    "    km.fit(dataStd)\n",
    "    WSSs.append(km.inertia_)\n",
    "WSSs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, 15), WSSs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con K=3 encontramos aproximadamente el codo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silueta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos ahora con el método de silueta cuántos clusters deberíamos tener. Obtengamos las siluetas para k = 2, 3, 4 y 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos las siluetas de los puntos de cada cluster.\n",
    "Vamos a crear un bar plot horizontal (barh) para los puntos de cada cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=2\n",
    "kmeans = KMeans(n_clusters=k, random_state=0, n_init=10)\n",
    "kmeans.fit(dataStd)\n",
    "y_clusters = kmeans.labels_\n",
    "cluster_labels = np.unique(y_clusters)\n",
    "\n",
    "silueta_puntos= silhouette_samples(dataStd, y_clusters, metric='euclidean')\n",
    "\n",
    "y_ax_lower, y_ax_upper = 0, 0\n",
    "yticks = []\n",
    "colores = ['r', 'g', 'b', 'y', 'o']\n",
    "for i, c in enumerate(cluster_labels):\n",
    "    silueta_puntos_c = silueta_puntos[y_clusters == c]\n",
    "    silueta_puntos_c.sort()\n",
    "    y_ax_upper += len(silueta_puntos_c)\n",
    "    color = colores[i]\n",
    "    plt.barh(range(y_ax_lower, y_ax_upper), silueta_puntos_c, height=1.0, \n",
    "             edgecolor='none', color=color)\n",
    "\n",
    "    yticks.append((y_ax_lower + y_ax_upper) / 2.)\n",
    "    y_ax_lower += len(silueta_puntos_c)\n",
    "    \n",
    "silueta_promedio = np.mean(silueta_puntos)\n",
    "plt.axvline(silueta_promedio, color=\"black\", linestyle=\"--\") \n",
    "\n",
    "plt.yticks(yticks, cluster_labels + 1)\n",
    "plt.ylabel('Cluster')\n",
    "plt.xlabel('Coeficiente de silueta')\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('./figures/silhouette.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=4\n",
    "kmeans = KMeans(n_clusters=k, random_state=0, n_init=10)\n",
    "kmeans.fit(dataStd)\n",
    "y_clusters = kmeans.labels_\n",
    "cluster_labels = np.unique(y_clusters)\n",
    "\n",
    "silueta_puntos= silhouette_samples(dataStd, y_clusters, metric='euclidean')\n",
    "\n",
    "y_ax_lower, y_ax_upper = 0, 0\n",
    "yticks = []\n",
    "colores = ['r', 'g', 'b', 'y', 'o']\n",
    "for i, c in enumerate(cluster_labels):\n",
    "    silueta_puntos_c = silueta_puntos[y_clusters == c]\n",
    "    silueta_puntos_c.sort()\n",
    "    y_ax_upper += len(silueta_puntos_c)\n",
    "    color = colores[i]\n",
    "    plt.barh(range(y_ax_lower, y_ax_upper), silueta_puntos_c, height=1.0, \n",
    "             edgecolor='none', color=color)\n",
    "\n",
    "    yticks.append((y_ax_lower + y_ax_upper) / 2.)\n",
    "    y_ax_lower += len(silueta_puntos_c)\n",
    "    \n",
    "silueta_promedio = np.mean(silueta_puntos)\n",
    "plt.axvline(silueta_promedio, color=\"black\", linestyle=\"--\") \n",
    "\n",
    "plt.yticks(yticks, cluster_labels + 1)\n",
    "plt.ylabel('Cluster')\n",
    "plt.xlabel('Coeficiente de silueta')\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('./figures/silhouette.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=5\n",
    "kmeans = KMeans(n_clusters=k, random_state=0, n_init=10)\n",
    "kmeans.fit(dataStd)\n",
    "y_clusters = kmeans.labels_\n",
    "cluster_labels = np.unique(y_clusters)\n",
    "\n",
    "silueta_puntos= silhouette_samples(dataStd, y_clusters, metric='euclidean')\n",
    "\n",
    "y_ax_lower, y_ax_upper = 0, 0\n",
    "yticks = []\n",
    "colores = ['red', 'g', 'b', 'y', 'darkorange']\n",
    "for i, c in enumerate(cluster_labels):\n",
    "    silueta_puntos_c = silueta_puntos[y_clusters == c]\n",
    "    silueta_puntos_c.sort()\n",
    "    y_ax_upper += len(silueta_puntos_c)\n",
    "    color = colores[i]\n",
    "    plt.barh(range(y_ax_lower, y_ax_upper), silueta_puntos_c, height=1.0, \n",
    "             edgecolor='none', color=color)\n",
    "\n",
    "    yticks.append((y_ax_lower + y_ax_upper) / 2.)\n",
    "    y_ax_lower += len(silueta_puntos_c)\n",
    "    \n",
    "silueta_promedio = np.mean(silueta_puntos)\n",
    "plt.axvline(silueta_promedio, color=\"black\", linestyle=\"--\") \n",
    "\n",
    "plt.yticks(yticks, cluster_labels + 1)\n",
    "plt.ylabel('Cluster')\n",
    "plt.xlabel('Coeficiente de silueta')\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('./figures/silhouette.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el método de silueta lo más indicado podría ser solo considerar dos clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calinski-Harabaz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intentemos ahora con la métrica de Calinski-Harabasz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHs = []\n",
    "for i in range(2,15) :\n",
    "    km = KMeans(n_clusters=i, random_state=0)\n",
    "    km.fit(dataStd)\n",
    "    CH = calinski_harabaz_score(dataStd, km.labels_) \n",
    "    CHs.append(CH)\n",
    "CHs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(2, 15), CHs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este criterio, el mayor score está en K=2, al igual que con el método de silueta. Pero como ya dijimos, no nos convendría mucho un clustering con solo dos segmentos para este caso de aplicación.\n",
    "Encontramos que entre mas pequeño el K mejor, pero vemos que después de K=6 encontramos una gran desmejoría."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 5. Reducción de dimensionalidad con PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a buscar una mejor representación de los datos que nos permita conservar la mayor cantidad de información a través de la transformación de las 6 variables originales en componentes principales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(dataStd.iloc[:, 0:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez ajustado el objeto PCA a un dataset, este permite acceder a diferentes aspectos resultantes de la transformación:\n",
    "- components_: los ejes de los componentes principales en función de las variables originales. Como teníamos 6 variables, vamos a tener 6 PCs, cada uno con las cargas (*loadings*) correspondientes a cada variable original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- explained_variance_: la varianza explicada por cada eje en las unidades originales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- explained_variance_ratio_: la proporción de la varianza explicada por cada eje, en porcentaje (la suma da 100%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_exp=pca.explained_variance_ratio_ # varianza explicada por cada PC\n",
    "cum_var_exp = np.cumsum(var_exp) # varianza acumulada por los primeros n PCs\n",
    "var_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objeto PCA sirve además para pasar de la representación en las dimensiones originales a la de las dimensiones en el espacio de los componentes principales encontrados, a partir de su método transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPca = pca.transform(dataStd.iloc[:, 0:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos gráficamente la cantidad de información correspondiente a cada componente principal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.bar(range(len(var_exp)), var_exp, alpha=0.3333, align='center', label='Varianza explicada por cada PC', color = 'g')\n",
    "plt.step(range(len(cum_var_exp)), cum_var_exp, where='mid',label='Varianza explicada acumulada')\n",
    "plt.ylabel('Porcentaje de varianza explicada')\n",
    "plt.xlabel('Componentes principales')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(pca.explained_variance_ratio_[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontramos que los primeros 3 componentes conservan el 81.6% de la información original, y los primeros 4 el 93.2%.\n",
    "Vamos a quedarnos solo con los 3 primeros PCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPca = dataPca[:,0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPca[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver los puntos en el nuevo sistema de representación dado por los componentes principales.\n",
    "Creamos una función que permite plotear tanto los puntos de los datos como los loadings de las variables originales (tomada de https://stackoverflow.com/questions/39216897/plot-pca-loadings-and-loading-in-biplot-in-sklearn-like-rs-autoplot).\n",
    "Esto nos permitirá entender mejor la relación entre componentes principales y variables originales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biplot(data, loadings, index1, index2, labels=None):\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    xs = data[:,index1]\n",
    "    ys = data[:,index2]\n",
    "    n=loadings.shape[0]\n",
    "    scalex = 1.0/(xs.max()- xs.min())\n",
    "    scaley = 1.0/(ys.max()- ys.min())\n",
    "    plt.scatter(xs*scalex,ys*scaley)\n",
    "    for i in range(n):\n",
    "        plt.arrow(0, 0, loadings[i,index1], loadings[i,index2],color='r',alpha=0.5)\n",
    "        if labels is None:\n",
    "            plt.text(loadings[i,index1]* 1.15, loadings[i,index2] * 1.15, \"Var\"+str(i+1), color='g', ha='center', va='center')\n",
    "        else:\n",
    "            plt.text(loadings[i,index1]* 1.15, loadings[i,index2] * 1.15, labels[i], color='g', ha='center', va='center')\n",
    "    plt.xlim(-1,1)\n",
    "    plt.ylim(-1,1)\n",
    "    plt.xlabel(\"PC{}\".format(index1))\n",
    "    plt.ylabel(\"PC{}\".format(index2))\n",
    "    plt.grid() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos como nos va con los primeros dos componentes principales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biplot(dataPca, pca.components_, 0, 1, ['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biplot(dataPca, pca.components_, 0, 2, ['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataStd.columns[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos decir que:\n",
    "- El componente PC1 representa positivamente las compras de leche en su sentido positivo, y negativamente las compras en Groceries y Frozen. Las otras variables no tienen mayor incidencia.\n",
    "- El componente PC2 representa sobretodo las compras de Detergentes/Papel y Fresh (positivamente)\n",
    "- El componente PC3 representa sobretodo las compras de Delicatessen y Fresh (positivamente), y Detergentes/Papel y Frozen (negativamente)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que ya entendemos el significado de los componentes principales, podemos proseguir a un clustering de los registros en el espacio reducido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPca = pd.DataFrame(dataPca)\n",
    "dataPca.columns=['PC1', 'PC2', 'PC3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3, random_state=0, n_init=10)\n",
    "kmeans.fit(dataPca)\n",
    "clusters = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPca['Cluster']= clusters\n",
    "counter=Counter(clusters)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,15))\n",
    "i=1\n",
    "for var in dataPca.columns[0:3]:\n",
    "    ax = fig.add_subplot(math.ceil(len(var_num)/2), 2, i)\n",
    "    sns.kdeplot(dataPca.loc[dataPca.Cluster==0][var], shade=True, color='r', ax=ax);\n",
    "    sns.kdeplot(dataPca.loc[dataPca.Cluster==1][var], shade=True, color='b', ax=ax);\n",
    "    sns.kdeplot(dataPca.loc[dataPca.Cluster==2][var], shade=True, color='g', ax=ax);\n",
    "    plt.title(var)\n",
    "    plt.legend(['Cluster 0', 'Cluster 1', 'Cluster 2'])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que con K=3, El PC1 sirve para separar bien los puntos del cluster rojo (0), el PC2 sirve para distinguir el cluster verde (2). El cluster azul (1) no se puede separar directamente de los demas a través de uno de los PCs, pero si al considerar los 3 PCs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamoslos en scatterplots para entender mejor las diferencias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10))\n",
    "colorPalette = [\"r\", \"b\", \"g\"]\n",
    "ax = fig.add_subplot(2, 2, 1)\n",
    "sns.scatterplot(x=\"PC1\", y=\"PC2\", hue=\"Cluster\", data=dataPca, ax=ax, palette=colorPalette, s=100, alpha=0.5)\n",
    "plt.title(\"PC1 vs. PC2\")\n",
    "ax = fig.add_subplot(2, 2, 2)\n",
    "sns.scatterplot(x=\"PC1\", y=\"PC3\", hue=\"Cluster\", data=dataPca, ax=ax, palette=colorPalette, s=100, alpha=0.5)\n",
    "plt.title(\"PC1 vs. PC3\")\n",
    "ax = fig.add_subplot(2, 2, 3)\n",
    "sns.scatterplot(x=\"PC2\", y=\"PC3\", hue=\"Cluster\", data=dataPca, ax=ax, palette=colorPalette, s=100, alpha=0.5)\n",
    "plt.title(\"PC2 vs. PC3\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que con K=3, en el plot de los 2 primeros PCs podemos separar bien los 3 clusters.\n",
    "Recordemos que el PC1 representa positivamente las compras de leche en su sentido positivo, y negativamente las compras en Groceries y Frozen, y que el componente PC2 representa sobretodo las compras de Detergentes/Papel y Fresh (positivamente)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota**: Realizar la determinación del número de cluster puede hacerse tanto en el espacio de representación original (ya estandarizado) como en el de los componentes principales (considerandolos todos). Los resultados serán los mismos, ya que tanto el método del codo como el de la silueta se basan en cálculos de distancias, que se conservan después de la transformación en componentes temporales, que no es más que una rotación de los ejes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
